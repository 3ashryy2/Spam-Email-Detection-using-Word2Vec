{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Email Classification Report\n",
    "Overview\n",
    "This project aims to develop a spam email classification system using various natural language processing (NLP) and machine learning techniques. The code provided outlines the entire pipeline from data preprocessing to model training and evaluation. The dataset used in this project is labeled as \"Spam_Email_Data.csv\", which consists of emails classified into spam and non-spam categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Spam_Email_Data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "Data preprocessing is a crucial step in any machine learning task. In this project, the following preprocessing steps were applied to the email text data:\n",
    "\n",
    "- Cleaning Text: The raw email texts were cleaned to remove headers, HTML tags, email addresses, URLs, non-word characters, and extra whitespaces.\n",
    "- Text Normalization:\n",
    "Conversion to lowercase to ensure uniformity.\n",
    "Removal of non-alphabet characters.\n",
    "Tokenization of text into individual words using NLTK's word_tokenize.\n",
    "Removal of English stopwords using NLTKâ€™s predefined list.\n",
    "Lemmatization of words to reduce them to their base or root form.\n",
    "- Final Text Representation: After lemmatization, the tokens were joined back into a single string per email, which serves as the final text input for vectorization and machine learning modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    From Mon Jul 29 112802 2002 Return-Path Delive...\n",
      "1    From Mon Jun 24 175421 2002 Return-Path Delive...\n",
      "2    From Mon Jul 29 113957 2002 Return-Path Delive...\n",
      "3    From Mon Jun 24 174923 2002 Return-Path Delive...\n",
      "4    From Mon Aug 19 110247 2002 Return-Path Delive...\n",
      "Name: cleaned_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_email_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    text= re.sub(r'^(.*?\\n\\n)', '', text, flags=re.S)      #remove email headers\n",
    "    # remove html tags\n",
    "    text= re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "    text= re.sub(r'\\S*@\\S*\\s?', '', text)  # remove emails\n",
    "    text= re.sub(r'http\\S+', '', text)     # remove urls\n",
    "\n",
    "    #remove all non alphanum\n",
    "    text= re.sub(r'[^\\w\\s.,;!?-]', '', text)\n",
    "\n",
    "    #cleaning white spaces\n",
    "    text= re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "data['cleaned_text']= data['text'].apply(clean_email_text)\n",
    "\n",
    "\n",
    "print(data[ 'cleaned_text'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  From ilug-admin@linux.ie Mon Jul 29 11:28:02 2...   \n",
      "1  From gort44@excite.com Mon Jun 24 17:54:21 200...   \n",
      "2  From fork-admin@xent.com Mon Jul 29 11:39:57 2...   \n",
      "3  From dcm123@btamail.net.cn Mon Jun 24 17:49:23...   \n",
      "4  From ilug-admin@linux.ie Mon Aug 19 11:02:47 2...   \n",
      "\n",
      "                                        cleaned_text  \\\n",
      "0  from mon jul 29 112802 2002 return-path delive...   \n",
      "1  from mon jun 24 175421 2002 return-path delive...   \n",
      "2  from mon jul 29 113957 2002 return-path delive...   \n",
      "3  from mon jun 24 174923 2002 return-path delive...   \n",
      "4  from mon aug 19 110247 2002 return-path delive...   \n",
      "\n",
      "                                       filtered_text  \\\n",
      "0  [mon, jul, 29, 112802, 2002, return-path, deli...   \n",
      "1  [mon, jun, 24, 175421, 2002, return-path, deli...   \n",
      "2  [mon, jul, 29, 113957, 2002, return-path, deli...   \n",
      "3  [mon, jun, 24, 174923, 2002, return-path, deli...   \n",
      "4  [mon, aug, 19, 110247, 2002, return-path, deli...   \n",
      "\n",
      "                                     lemmatized_text  \n",
      "0  [from, mon, jul, 29, 112802, 2002, return-path...  \n",
      "1  [from, mon, jun, 24, 175421, 2002, return-path...  \n",
      "2  [from, mon, jul, 29, 113957, 2002, return-path...  \n",
      "3  [from, mon, jun, 24, 174923, 2002, return-path...  \n",
      "4  [from, mon, aug, 19, 110247, 2002, return-path...  \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# lowercase\n",
    "data['cleaned_text']= data['cleaned_text'].apply(lambda x: x.lower())\n",
    "\n",
    "#tokenize the text\n",
    "data['tokenized_text'] = data['cleaned_text'].apply(word_tokenize)\n",
    "\n",
    "#remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "data['filtered_text'] = data['tokenized_text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "#apply lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "data['lemmatized_text'] = data['tokenized_text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "# Check the output\n",
    "print(data[['text', 'cleaned_text', 'filtered_text', 'lemmatized_text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       from mon jul 29 112802 2002 return-path delive...\n",
      "1       from mon jun 24 175421 2002 return-path delive...\n",
      "2       from mon jul 29 113957 2002 return-path delive...\n",
      "3       from mon jun 24 174923 2002 return-path delive...\n",
      "4       from mon aug 19 110247 2002 return-path delive...\n",
      "                              ...                        \n",
      "5791    from mon jul 22 181245 2002 return-path delive...\n",
      "5792    from mon oct 7 203702 2002 return-path deliver...\n",
      "5793    received from hq.pro-ns.net localhost 127.0.0....\n",
      "5794    from thu sep 12 184430 2002 return-path delive...\n",
      "5795    from mon sep 30 134410 2002 return-path delive...\n",
      "Name: final_text, Length: 5796, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data['final_text'] = data['lemmatized_text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "print(data['final_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word2Vec: Custom transformer using Gensim's Word2Vec model to convert words into vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34793990, 155067240)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = data['final_text'].tolist()\n",
    "word2vec_model = Word2Vec(sentences=tokens, vector_size=100, window=5, min_count=2, workers=4)\n",
    "word2vec_model.train(tokens, total_examples=len(tokens), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_word2vec(tokens,model,vector_size):\n",
    "    vec= np.zeros((vector_size,), dtype='float32')\n",
    "    n_words= 0\n",
    "    for word in tokens:\n",
    "        if word in model.wv:\n",
    "            n_words += 1\n",
    "            vec= np.add(vec,model.wv[word])\n",
    "    if n_words > 0:\n",
    "        vec= np.divide(vec,n_words)\n",
    "    return vec\n",
    "\n",
    "vector_size = 100\n",
    "data['word2vec_features'] = data['final_text'].apply(lambda x: get_average_word2vec(x, word2vec_model, vector_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= np.array(data['word2vec_features'].tolist())\n",
    "y= data['target'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.12.2-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\python312\\lib\\site-packages (from imbalanced-learn) (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\dell\\appdata\\roaming\\python\\python312\\site-packages (from imbalanced-learn) (1.11.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\dell\\appdata\\roaming\\python\\python312\\site-packages (from imbalanced-learn) (1.3.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\dell\\appdata\\roaming\\python\\python312\\site-packages (from imbalanced-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\dell\\appdata\\roaming\\python\\python312\\site-packages (from imbalanced-learn) (3.2.0)\n",
      "Downloading imbalanced_learn-0.12.2-py3-none-any.whl (257 kB)\n",
      "   ---------------------------------------- 0.0/258.0 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/258.0 kB ? eta -:--:--\n",
      "   ---- ---------------------------------- 30.7/258.0 kB 435.7 kB/s eta 0:00:01\n",
      "   ------------- ------------------------- 92.2/258.0 kB 871.5 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 204.8/258.0 kB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 258.0/258.0 kB 1.4 MB/s eta 0:00:00\n",
      "Installing collected packages: imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.12.2\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled: 0    1515\n",
      "1    1515\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "# undersampler\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "\n",
    "#undersampling to the training data\n",
    "X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"Resampled: {pd.Series(y_train_resampled).value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building and Evaluation\n",
    "machine learning model built using vectorizer (Word2Vec)  Support Vector Machine (SVM). The evaluation of models was performed using a hold-out test set (20% of the data).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96       779\n",
      "           1       0.93      0.90      0.91       381\n",
      "\n",
      "    accuracy                           0.94      1160\n",
      "   macro avg       0.94      0.93      0.94      1160\n",
      "weighted avg       0.94      0.94      0.94      1160\n",
      "\n",
      "Confusion Matrix:\n",
      " [[753  26]\n",
      " [ 39 342]]\n",
      "ROC-AUC Score: 0.9777441972513384\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "svm_model = SVC(kernel='linear', probability=True)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svm_model.predict(X_test)\n",
    "y_pred_proba = svm_model.predict_proba(X_test)[:, 1]\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('Confusion Matrix:\\n', confusion_matrix(y_test, y_pred))\n",
    "print('ROC-AUC Score:', roc_auc_score(y_test, y_pred_proba))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
